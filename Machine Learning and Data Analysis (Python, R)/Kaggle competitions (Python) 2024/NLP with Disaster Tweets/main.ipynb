{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nAfter first attempts, the score was very low. (about 0.5)\n\nThen, we lokked up other Kaggler's submissions, in particular Beatriz Justino's https://www.kaggle.com/code/beatrizjustino/lstm-disaster-tweets. (Version 12)\n\nAnd we wondered, why it was so much better (about 0.8). Here are the findngs:","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\n\n# Imports that Justino's code needs:\nfrom keras import preprocessing\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers import Embedding, LSTM, Dense, Dropout,BatchNormalization\nfrom keras.models import Sequential\nimport tensorflow as tf\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:44:52.838753Z","iopub.execute_input":"2024-08-13T11:44:52.839885Z","iopub.status.idle":"2024-08-13T11:45:12.017123Z","shell.execute_reply.started":"2024-08-13T11:44:52.839848Z","shell.execute_reply":"2024-08-13T11:45:12.015672Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-08-13 11:44:56.680066: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-13 11:44:56.680234: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-13 11:44:56.875226: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Choices","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 32\nMAX_TOKENS = 10000\nEPOCHS = 20\nMAX_SENTENCE_LENGTH = 60","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:45:12.020158Z","iopub.execute_input":"2024-08-13T11:45:12.021425Z","iopub.status.idle":"2024-08-13T11:45:12.027539Z","shell.execute_reply.started":"2024-08-13T11:45:12.021311Z","shell.execute_reply":"2024-08-13T11:45:12.026015Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Raw Datasets","metadata":{}},{"cell_type":"code","source":"all_known_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\ntrain_df, val_df = train_test_split(all_known_df, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:45:12.030070Z","iopub.execute_input":"2024-08-13T11:45:12.030859Z","iopub.status.idle":"2024-08-13T11:45:12.172135Z","shell.execute_reply.started":"2024-08-13T11:45:12.030812Z","shell.execute_reply":"2024-08-13T11:45:12.170016Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Note that the data seems to be badly classified:\nTwo examples classified as disaster, which I stumbled about:","metadata":{}},{"cell_type":"code","source":"badly_classified_example = all_known_df[all_known_df[\"text\"].str.contains(\"whirlwind of time\")]\nprint(badly_classified_example)\nprint(badly_classified_example['text'].to_numpy())\nprint(badly_classified_example['target'].to_numpy())\n\nbadly_classified_example = all_known_df[all_known_df[\"text\"].str.contains(\"Attack II Volleyball\")]\nprint()\nprint(print(badly_classified_example))\nprint(badly_classified_example['text'].to_numpy())\nprint(badly_classified_example['target'].to_numpy())","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:45:12.175667Z","iopub.execute_input":"2024-08-13T11:45:12.176176Z","iopub.status.idle":"2024-08-13T11:45:12.214927Z","shell.execute_reply.started":"2024-08-13T11:45:12.176137Z","shell.execute_reply":"2024-08-13T11:45:12.212867Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"         id    keyword                        location  \\\n7270  10409  whirlwind  Stamford & Cork (& Shropshire)   \n\n                                                   text  target  \n7270  I moved to England five years ago today. What ...       1  \n['I moved to England five years ago today. What a whirlwind of time it has been! http://t.co/eaSlGeA1B7']\n[1]\n\n      id keyword location                                               text  \\\n466  674  attack      NaN  #volleyball Attack II Volleyball Training Mach...   \n\n     target  \n466       1  \nNone\n['#volleyball Attack II Volleyball Training Machine - Sets Simulation - http://t.co/dCDeCFv934 http://t.co/dWBC1dUvdk']\n[1]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Justino's Raw Dataset\n\nJustino encodes the target differently, and balances the uneven classes. (It does not seem to help a lot.)","metadata":{}},{"cell_type":"code","source":"def get_Justino_raw_data(df, df_val=None):\n    def extract_text_and_target(df):\n        return df[\"text\"].to_numpy(), df[\"target\"].to_numpy()\n    class_0 = df[df['target'] == 0]\n    class_1 = df[df['target'] == 1]\n    class_0_under = class_0.sample(len(class_1), random_state=42)\n    df_equalized = pd.concat([class_0_under, class_1])\n    text, label = extract_text_and_target(df_equalized)\n    if df_val is not None:\n        text_val, label_val = extract_text_and_target(df_val)\n        return text, label, text_val, label_val\n    else:\n        return text, label","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:45:12.217265Z","iopub.execute_input":"2024-08-13T11:45:12.218080Z","iopub.status.idle":"2024-08-13T11:45:12.231814Z","shell.execute_reply.started":"2024-08-13T11:45:12.218006Z","shell.execute_reply":"2024-08-13T11:45:12.229904Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Our preprocessing functions","metadata":{}},{"cell_type":"markdown","source":"One function for cleaning text snippets:","metadata":{}},{"cell_type":"code","source":"import re, string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nstemmer = SnowballStemmer('english')\nstop_words = set(stopwords.words('english'))\n\ndef clean_stem_text(text):    \n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    # Remove @mentions and hashtags\n    text = re.sub(r'\\@\\w+|\\#', '', text)\n    # Remove digits\n    text = re.sub(r'\\d+', '', text)\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    # Remove extra spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    # Convert to lowercase\n    text = text.lower()\n    # Remove stopwords and stem\n    text = ' '.join([stemmer.stem(word) \n                     for word in text.split() \n                     if word not in stop_words])\n    return text\n\ndef clean_stem_byte_tensor(tensor):\n    texts = tensor.numpy()\n    texts = np.reshape(texts, (-1,))\n    texts = [text.decode(\"utf-8\") for text in texts]\n    texts = [clean_stem_text(text) for text in texts]\n    return tf.constant(texts, dtype=tf.string, shape=(len(texts),))","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:45:12.233976Z","iopub.execute_input":"2024-08-13T11:45:12.234606Z","iopub.status.idle":"2024-08-13T11:45:13.127469Z","shell.execute_reply.started":"2024-08-13T11:45:12.234550Z","shell.execute_reply":"2024-08-13T11:45:13.125509Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Lots of functions for applying the cleaning function and for encoding the text numerically:\n\n(One can later chose from these different methods.)","metadata":{}},{"cell_type":"code","source":"int_text_vectorization = keras.layers.TextVectorization(\n    output_mode=\"int\",\n    max_tokens=MAX_TOKENS,\n    output_sequence_length=MAX_SENTENCE_LENGTH\n)\nint_text_vectorization_custom_clean_text = keras.layers.TextVectorization(\n    output_mode=\"int\",\n    max_tokens=MAX_TOKENS,\n    output_sequence_length=MAX_SENTENCE_LENGTH,\n    standardize=clean_stem_byte_tensor\n)\ntf_idf_text_vectorization_custom_clean_text = keras.layers.TextVectorization(\n    output_mode=\"tf_idf\",\n    max_tokens=MAX_TOKENS,\n    standardize=clean_stem_byte_tensor\n)\ntf_idf_text_vectorization = keras.layers.TextVectorization(\n    output_mode=\"tf_idf\",\n    max_tokens=MAX_TOKENS\n)\ntf_idf_text_vectorization_custom_clean_text_ngram_2 = keras.layers.TextVectorization(\n    output_mode=\"tf_idf\",\n    max_tokens=MAX_TOKENS,\n    standardize=clean_stem_byte_tensor,\n    ngrams=(1,2)\n)\nint_text_vectorization_custom_clean_text_ngram_2 = keras.layers.TextVectorization(\n    output_mode=\"int\",\n    max_tokens=MAX_TOKENS,\n    standardize=clean_stem_byte_tensor,\n    ngrams=(1,2)\n)\n\ndef preprocess_df(df, df_val=None, mode=\"int\", clean=\"standard\", ngrams=(1,), df_test=None, max_tokens=MAX_TOKENS,\n                 output_sequence_length=MAX_SENTENCE_LENGTH):\n    assert mode in (\"int\", \"tf_idf\")\n    assert clean in (\"standard\", \"custom\")\n    assert ngrams in [(1,), (1,2)]\n    \n    if clean==\"standard\":\n        clean_method = \"lower_and_strip_punctuation\"\n    elif clean==\"custom\":\n        clean_method = clean_stem_byte_tensor\n    \n    # Need to silence output_sequence_length for the TextVectorization-layer if not in \"int\"-mode:\n    if mode!=\"int\": output_sequence_length=None\n    \n    vectorizer = keras.layers.TextVectorization(\n        output_mode=mode,\n        max_tokens=max_tokens,\n        output_sequence_length=output_sequence_length,\n        standardize=clean_method\n    )\n        \n    def df_to_X_y(df):\n        X = df[\"text\"].to_numpy()\n        y = np.reshape(np.array(df[\"target\"]), (-1,1))\n        return X, y\n    \n    X, y = df_to_X_y(df)\n    vectorizer.adapt(X)\n    if df_test is not None:\n        X_test = vectorizer(df_test[\"text\"].to_numpy())\n        if df_val is not None:\n            X_val, y_val = df_to_X_y(df_val)\n            return vectorizer(X), y, vectorizer(X_val), y_val, X_test\n        else:\n            return vectorizer(X), y, X_test\n    else:\n        if X_val is not None:\n            X_val, y_val = df_to_X_y(df_val)\n            return vectorizer(X), y, vectorizer(X_val), y_val\n        else:\n            return vectorizer(X), y","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:45:13.130354Z","iopub.execute_input":"2024-08-13T11:45:13.131590Z","iopub.status.idle":"2024-08-13T11:45:13.305755Z","shell.execute_reply.started":"2024-08-13T11:45:13.131528Z","shell.execute_reply":"2024-08-13T11:45:13.303626Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"(The proprocessing is safely run on the dataset (not in the model) to save the GPU from being blocked.)","metadata":{}},{"cell_type":"markdown","source":"# Justino's preprocessing functions\nATTENTION: There seemed to be a bug in applying the stemming! It does not work with lists, but should be applied on single words.","metadata":{}},{"cell_type":"code","source":"def clean_Justino(text_array):\n    def process_sentence(sentence):\n        sentence = re.sub(r'http\\S+|www\\S+|https\\S+', '', sentence, flags=re.MULTILINE)\n        sentence = re.sub(r'#\\S+', '', sentence)\n        sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n        words = sentence.split()\n        filtered_words = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]\n        return ' '.join(filtered_words)\n    return [process_sentence(sentence) for sentence in text_array]\n\ndef get_Justino_preprocessed_data(df, df_val=None, label_to_cat=True, vocab_length=None, length_long_sentence=MAX_SENTENCE_LENGTH):\n    if df_val is not None:\n        texts, labels, texts_val, labels_val = get_Justino_raw_data(df, df_val)\n        texts_val = clean_Justino(texts_val)\n        texts_val = [stemmer.stem(word) for word in texts_val]\n    else:\n        texts, labels = get_Justino_raw_data(df)\n    texts = clean_Justino(texts)\n    #print(texts[:5])\n    texts = [stemmer.stem(word) for word in texts]\n    #print(\"-----------\")\n    #print(texts[:5])\n    \n    word_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_length)\n    word_tokenizer.fit_on_texts(texts)\n    \n    #from nltk.tokenize import word_tokenize\n    #longest_train = max(text, key=lambda sentence: len(word_tokenize(sentence)))\n    #length_long_sentence = len(word_tokenize(longest_train))\n    \n    def process_data(texts, labels):\n        sequences = word_tokenizer.texts_to_sequences(texts)\n        padded_sequences = pad_sequences(sequences, maxlen=length_long_sentence)\n        if label_to_cat:\n            labels = keras.utils.to_categorical(labels)\n        return padded_sequences, labels\n    \n    padded_sequences, labels = process_data(texts, labels)\n    if df_val is not None:\n        padded_sequences_val, labels_val = process_data(texts_val, labels_val)\n        return padded_sequences, labels, padded_sequences_val, labels_val\n    else:\n        return padded_sequences, labels\n#get_Justino_preprocessed_data(train_df, val_df)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:45:13.307464Z","iopub.execute_input":"2024-08-13T11:45:13.307877Z","iopub.status.idle":"2024-08-13T11:45:13.326411Z","shell.execute_reply.started":"2024-08-13T11:45:13.307845Z","shell.execute_reply":"2024-08-13T11:45:13.324552Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# F1-Metric\n\nUsing metrics=[\"f1_score\"] is not working properly in tensorflow up until now!! The problem is discussed on some GitHub-site...\n\nWe specify an f1-metric more manually here:","metadata":{}},{"cell_type":"code","source":"f1 = tf.keras.metrics.F1Score(\n    average=None, threshold=0.5, name='f1', dtype=None\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:45:13.328785Z","iopub.execute_input":"2024-08-13T11:45:13.329426Z","iopub.status.idle":"2024-08-13T11:45:13.347336Z","shell.execute_reply.started":"2024-08-13T11:45:13.329374Z","shell.execute_reply":"2024-08-13T11:45:13.345872Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"For training, one could also try to use an adapted \"f1-loss\":","metadata":{}},{"cell_type":"code","source":"def f1_loss(y_true, y_pred):\n    \n    tp = tf.sum(tf.cast(y_true*y_pred, 'float'), axis=0)\n    tn = tf.sum(tf.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = tf.sum(tf.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = tf.sum(tf.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + tf.epsilon())\n    r = tp / (tp + fn + tf.epsilon())\n\n    f1 = 2*p*r / (p+r+tf.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return 1 - tf.mean(f1)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:45:13.351566Z","iopub.execute_input":"2024-08-13T11:45:13.352858Z","iopub.status.idle":"2024-08-13T11:45:13.364377Z","shell.execute_reply.started":"2024-08-13T11:45:13.352820Z","shell.execute_reply":"2024-08-13T11:45:13.362974Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Our Models","metadata":{}},{"cell_type":"code","source":"\ndef get_model(max_tokens=MAX_TOKENS, max_sentence_length=MAX_SENTENCE_LENGTH,\n              hidden_dim=16, embedding_dim=33, mode=\"int\"):\n    if mode==\"int\":\n        inputs = keras.Input(shape=(max_sentence_length,), dtype=\"int64\")\n        x = keras.layers.Embedding(input_dim=max_tokens, output_dim=embedding_dim)(inputs)\n        x = keras.layers.GlobalAveragePooling1D()(x)\n        #x = keras.layers.Flatten()(x)\n    elif mode==\"tf_idf\":\n        inputs = keras.Input(shape=(max_tokens,), dtype=\"int64\")\n        x = inputs\n    \n    x = keras.layers.Dense(hidden_dim, activation=\"relu\")(x)\n    x = keras.layers.Dense(hidden_dim, activation=\"relu\")(x)+x\n    #x = keras.layers.Dense(hidden_dim, activation=\"relu\")(x)\n    \n    x = keras.layers.Dropout(0.5)(x)\n    x = keras.layers.Dense(1)(x)\n    \n    model = keras.Model(inputs=inputs, outputs=x)\n    model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=True),\n                 #loss=f1_loss,\n                 optimizer=\"rmsprop\",\n                 metrics=[\"accuracy\", f1],\n                 #run_eagerly=True\n                 )\n    return model\n\n# We should also try to do simple logistic regression, when we find the time for it:\n\n#def get_LR_model(max_tokens):\n#    inputs = keras.Input(shape=(MAX_SENTENCE_LENGTH,), dtype=\"int64\")\n#    x = tf.keras.layers.CategoryEncoding(\n#        num_tokens=max_tokens, output_mode='multi_hot', sparse=False\n#    )(inputs)\n#    x = keras.layers.Dense(1)(x)\n#    model = keras.Model(inputs=inputs, outputs=x)\n#    model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=True),\n#                 #loss=f1_loss,\n#                 optimizer=\"adam\",\n#                 metrics=[\"f1_score\", \"accuracy\", \"precision\", \"recall\"],\n#                 #run_eagerly=True\n#                 )\n#    return model\n\n#def get_tf_idf_LR_model(max_tokens):\n#    inputs = keras.Input(shape=(max_tokens,), dtype=\"int64\")\n#    x = keras.layers.Dense(1)(inputs)\n#    model = keras.Model(inputs=inputs, outputs=x)\n#    model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=True),\n#                 #loss=f1_loss,\n#                 optimizer=\"adam\",\n#                 metrics=[\"f1_score\", \"accuracy\", \"precision\", \"recall\"],\n#                 #run_eagerly=True\n#                 )\n#    return model","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:45:13.365749Z","iopub.execute_input":"2024-08-13T11:45:13.366143Z","iopub.status.idle":"2024-08-13T11:45:13.381519Z","shell.execute_reply.started":"2024-08-13T11:45:13.366103Z","shell.execute_reply":"2024-08-13T11:45:13.380059Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# That's basically Justino's model for our own data-formatting.. Maybe we'll use it one day.\n\n#def get_LSTM_model():\n#    inputs = keras.Input((MAX_SENTENCE_LENGTH,), dtype=tf.int64)\n#    x = keras.layers.Embedding(input_dim=MAX_TOKENS, output_dim=128)(inputs)\n#    x = keras.layers.Dropout(0.5)(x)\n#    x = keras.layers.LSTM(65)(x)\n#    x = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n#    model=keras.Model(inputs, x)\n#    model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False),\n#                  optimizer=\"Adamax\",\n#                  metrics=[\"accuracy\", \"f1_score\"],\n#                 #run_eagerly=True\n#                 )\n#    model.summary()\n#    return model\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:45:13.383452Z","iopub.execute_input":"2024-08-13T11:45:13.384884Z","iopub.status.idle":"2024-08-13T11:45:13.402532Z","shell.execute_reply.started":"2024-08-13T11:45:13.384832Z","shell.execute_reply":"2024-08-13T11:45:13.400468Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Jutino's Model","metadata":{}},{"cell_type":"code","source":"def get_Justino_model(max_words=MAX_TOKENS, max_len=MAX_SENTENCE_LENGTH):\n    model = Sequential()\n    model.add(Embedding(max_words, 128))\n    model.add(Dropout(0.5))\n    model.add(LSTM(64))\n    model.add(Dense(2, activation='sigmoid'))\n\n    model.compile(loss='BinaryCrossentropy', optimizer='Adamax', metrics=['accuracy', f1])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:45:13.404176Z","iopub.execute_input":"2024-08-13T11:45:13.404595Z","iopub.status.idle":"2024-08-13T11:45:13.423343Z","shell.execute_reply.started":"2024-08-13T11:45:13.404563Z","shell.execute_reply":"2024-08-13T11:45:13.421779Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Function for saving predictions","metadata":{}},{"cell_type":"code","source":"def predict_and_save(model, X_test, filename):\n    predictions_logits = model.predict(X_test)\n    predictions = tf.math.sigmoid(predictions_logits).numpy()\n    predictions = (predictions>0.5).astype(int)\n    df = test_df[[\"id\"]].copy()\n    df[\"target\"] = predictions\n    df.to_csv(filename, index=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:29:07.213460Z","iopub.execute_input":"2024-08-13T12:29:07.213981Z","iopub.status.idle":"2024-08-13T12:29:07.222502Z","shell.execute_reply.started":"2024-08-13T12:29:07.213946Z","shell.execute_reply":"2024-08-13T12:29:07.221023Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"# Runs","metadata":{}},{"cell_type":"markdown","source":"Justino orginal:","metadata":{}},{"cell_type":"code","source":"X_Justino, y_Justino = get_Justino_preprocessed_data(all_known_df)\nmod_Justino = get_Justino_model(max_words=np.max(np.max(X_Justino))+1)\nmod_Justino.fit(X_Justino, y_Justino, epochs=EPOCHS, batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:45:13.425479Z","iopub.execute_input":"2024-08-13T11:45:13.426036Z","iopub.status.idle":"2024-08-13T11:47:50.950567Z","shell.execute_reply.started":"2024-08-13T11:45:13.425986Z","shell.execute_reply":"2024-08-13T11:47:50.949195Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 68ms/step - accuracy: 0.5454 - f1: 0.5085 - loss: 0.6880\nEpoch 2/20\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 72ms/step - accuracy: 0.6532 - f1: 0.6616 - loss: 0.6234\nEpoch 3/20\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 70ms/step - accuracy: 0.7717 - f1: 0.7658 - loss: 0.4756\nEpoch 4/20\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 69ms/step - accuracy: 0.8253 - f1: 0.8246 - loss: 0.3982\nEpoch 5/20\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 72ms/step - accuracy: 0.8448 - f1: 0.8442 - loss: 0.3638\nEpoch 6/20\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 72ms/step - accuracy: 0.8624 - f1: 0.8625 - loss: 0.3226\nEpoch 7/20\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 67ms/step - accuracy: 0.8792 - f1: 0.8793 - loss: 0.2950\nEpoch 8/20\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 64ms/step - accuracy: 0.8882 - f1: 0.8876 - loss: 0.2739\nEpoch 9/20\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 72ms/step - accuracy: 0.8996 - f1: 0.8994 - loss: 0.2487\nEpoch 10/20\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.9100 - f1: 0.9098 - loss: 0.2199\nEpoch 11/20\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - accuracy: 0.9166 - f1: 0.9169 - loss: 0.2057\nEpoch 12/20\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 68ms/step - accuracy: 0.9247 - f1: 0.9245 - loss: 0.1885\nEpoch 13/20\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 69ms/step - accuracy: 0.9241 - f1: 0.9238 - loss: 0.1841\nEpoch 14/20\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 68ms/step - accuracy: 0.9381 - f1: 0.9382 - loss: 0.1609\nEpoch 15/20\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - accuracy: 0.9419 - f1: 0.9424 - loss: 0.1554\nEpoch 16/20\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 65ms/step - accuracy: 0.9426 - f1: 0.9426 - loss: 0.1445\nEpoch 17/20\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - accuracy: 0.9495 - f1: 0.9494 - loss: 0.1292\nEpoch 18/20\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - accuracy: 0.9482 - f1: 0.9486 - loss: 0.1381\nEpoch 19/20\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - accuracy: 0.9557 - f1: 0.9546 - loss: 0.1202\nEpoch 20/20\n\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - accuracy: 0.9607 - f1: 0.9609 - loss: 0.1045\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7d4e3c746800>"},"metadata":{}}]},{"cell_type":"markdown","source":"Justino with splitting data into train and val:","metadata":{}},{"cell_type":"code","source":"X_Justino, y_Justino, X_Justino_val, y_Justino_val = get_Justino_preprocessed_data(train_df, val_df)\nmod_Justino = get_Justino_model(max_words=np.max(np.max(X_Justino))+1)\nmod_Justino.fit(X_Justino, y_Justino, validation_data=(X_Justino_val, y_Justino_val), epochs=EPOCHS, batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:47:50.951977Z","iopub.execute_input":"2024-08-13T11:47:50.952320Z","iopub.status.idle":"2024-08-13T11:50:12.733856Z","shell.execute_reply.started":"2024-08-13T11:47:50.952290Z","shell.execute_reply":"2024-08-13T11:50:12.732010Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - accuracy: 0.5213 - f1: 0.8396 - loss: 0.6900 - val_accuracy: 0.6750 - val_f1: 0.6349 - val_loss: 0.6586\nEpoch 2/20\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - accuracy: 0.6416 - f1: 0.6213 - loss: 0.6502 - val_accuracy: 0.6809 - val_f1: 0.6799 - val_loss: 0.5909\nEpoch 3/20\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.7384 - f1: 0.7365 - loss: 0.5489 - val_accuracy: 0.7505 - val_f1: 0.7461 - val_loss: 0.5131\nEpoch 4/20\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - accuracy: 0.8239 - f1: 0.8231 - loss: 0.4218 - val_accuracy: 0.7827 - val_f1: 0.7666 - val_loss: 0.4765\nEpoch 5/20\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - accuracy: 0.8455 - f1: 0.8454 - loss: 0.3702 - val_accuracy: 0.7833 - val_f1: 0.7638 - val_loss: 0.4809\nEpoch 6/20\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - accuracy: 0.8570 - f1: 0.8561 - loss: 0.3375 - val_accuracy: 0.7820 - val_f1: 0.7749 - val_loss: 0.4843\nEpoch 7/20\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - accuracy: 0.8719 - f1: 0.8707 - loss: 0.3086 - val_accuracy: 0.7833 - val_f1: 0.7769 - val_loss: 0.4860\nEpoch 8/20\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - accuracy: 0.8899 - f1: 0.8894 - loss: 0.2750 - val_accuracy: 0.7879 - val_f1: 0.7782 - val_loss: 0.4991\nEpoch 9/20\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 70ms/step - accuracy: 0.9016 - f1: 0.9015 - loss: 0.2484 - val_accuracy: 0.7728 - val_f1: 0.7694 - val_loss: 0.5461\nEpoch 10/20\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - accuracy: 0.9125 - f1: 0.9127 - loss: 0.2166 - val_accuracy: 0.7741 - val_f1: 0.7702 - val_loss: 0.5456\nEpoch 11/20\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - accuracy: 0.9234 - f1: 0.9230 - loss: 0.2043 - val_accuracy: 0.7846 - val_f1: 0.7760 - val_loss: 0.5413\nEpoch 12/20\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - accuracy: 0.9318 - f1: 0.9311 - loss: 0.1864 - val_accuracy: 0.7774 - val_f1: 0.7732 - val_loss: 0.5692\nEpoch 13/20\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - accuracy: 0.9381 - f1: 0.9376 - loss: 0.1737 - val_accuracy: 0.7807 - val_f1: 0.7763 - val_loss: 0.5875\nEpoch 14/20\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 70ms/step - accuracy: 0.9402 - f1: 0.9401 - loss: 0.1599 - val_accuracy: 0.7800 - val_f1: 0.7753 - val_loss: 0.5975\nEpoch 15/20\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 70ms/step - accuracy: 0.9477 - f1: 0.9472 - loss: 0.1468 - val_accuracy: 0.7774 - val_f1: 0.7725 - val_loss: 0.5981\nEpoch 16/20\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - accuracy: 0.9522 - f1: 0.9512 - loss: 0.1328 - val_accuracy: 0.7722 - val_f1: 0.7694 - val_loss: 0.6476\nEpoch 17/20\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - accuracy: 0.9500 - f1: 0.9494 - loss: 0.1285 - val_accuracy: 0.7768 - val_f1: 0.7713 - val_loss: 0.6553\nEpoch 18/20\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - accuracy: 0.9557 - f1: 0.9564 - loss: 0.1240 - val_accuracy: 0.7663 - val_f1: 0.7646 - val_loss: 0.6905\nEpoch 19/20\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - accuracy: 0.9566 - f1: 0.9566 - loss: 0.1133 - val_accuracy: 0.7754 - val_f1: 0.7714 - val_loss: 0.7003\nEpoch 20/20\n\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - accuracy: 0.9616 - f1: 0.9612 - loss: 0.1081 - val_accuracy: 0.7754 - val_f1: 0.7706 - val_loss: 0.7024\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7d4e3c35ce20>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Own models:","metadata":{}},{"cell_type":"markdown","source":"Using standard cleaning and multi-hot-encodig:","metadata":{}},{"cell_type":"code","source":"X_train, y_train, X_val, y_val, X_test = preprocess_df(train_df, val_df, df_test=test_df, mode=\"int\", clean=\"standard\")\nmodel = get_model(MAX_TOKENS)\nmodel.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=64,\n         callbacks=[keras.callbacks.ModelCheckpoint(\"own_model_int_standard.keras\",save_best_only=True)])","metadata":{"execution":{"iopub.status.busy":"2024-08-13T11:50:12.735754Z","iopub.execute_input":"2024-08-13T11:50:12.737007Z","iopub.status.idle":"2024-08-13T11:50:32.672824Z","shell.execute_reply.started":"2024-08-13T11:50:12.736954Z","shell.execute_reply":"2024-08-13T11:50:32.671019Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5713 - f1: 0.4417 - loss: 0.6887 - val_accuracy: 0.5739 - val_f1: 0.0000e+00 - val_loss: 0.6827\nEpoch 2/20\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5528 - f1: 0.0000e+00 - loss: 0.6867 - val_accuracy: 0.5739 - val_f1: 0.0000e+00 - val_loss: 0.6820\nEpoch 3/20\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5587 - f1: 0.0000e+00 - loss: 0.6884 - val_accuracy: 0.5739 - val_f1: 0.0000e+00 - val_loss: 0.6798\nEpoch 4/20\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5735 - f1: 0.0000e+00 - loss: 0.6817 - val_accuracy: 0.5739 - val_f1: 0.0000e+00 - val_loss: 0.6774\nEpoch 5/20\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5678 - f1: 0.0000e+00 - loss: 0.6802 - val_accuracy: 0.5739 - val_f1: 0.0000e+00 - val_loss: 0.6821\nEpoch 6/20\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5602 - f1: 0.0000e+00 - loss: 0.6789 - val_accuracy: 0.5739 - val_f1: 0.0000e+00 - val_loss: 0.6659\nEpoch 7/20\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5658 - f1: 0.0000e+00 - loss: 0.6676 - val_accuracy: 0.5739 - val_f1: 0.0000e+00 - val_loss: 0.6558\nEpoch 8/20\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5639 - f1: 5.0277e-04 - loss: 0.6505 - val_accuracy: 0.5739 - val_f1: 0.0000e+00 - val_loss: 0.6363\nEpoch 9/20\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5685 - f1: 0.0421 - loss: 0.6359 - val_accuracy: 0.5739 - val_f1: 0.0000e+00 - val_loss: 0.6126\nEpoch 10/20\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6059 - f1: 0.2091 - loss: 0.6137 - val_accuracy: 0.6060 - val_f1: 0.6495 - val_loss: 0.7169\nEpoch 11/20\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6390 - f1: 0.3778 - loss: 0.5984 - val_accuracy: 0.6014 - val_f1: 0.1266 - val_loss: 0.6082\nEpoch 12/20\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6766 - f1: 0.4393 - loss: 0.5864 - val_accuracy: 0.5680 - val_f1: 0.6428 - val_loss: 0.7502\nEpoch 13/20\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6752 - f1: 0.5132 - loss: 0.5863 - val_accuracy: 0.7663 - val_f1: 0.6691 - val_loss: 0.5482\nEpoch 14/20\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7244 - f1: 0.5706 - loss: 0.5388 - val_accuracy: 0.7354 - val_f1: 0.7192 - val_loss: 0.6150\nEpoch 15/20\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7239 - f1: 0.5993 - loss: 0.5424 - val_accuracy: 0.6100 - val_f1: 0.1610 - val_loss: 0.6856\nEpoch 16/20\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7291 - f1: 0.5967 - loss: 0.5337 - val_accuracy: 0.7689 - val_f1: 0.6583 - val_loss: 0.5027\nEpoch 17/20\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7503 - f1: 0.6253 - loss: 0.5077 - val_accuracy: 0.7820 - val_f1: 0.7009 - val_loss: 0.5008\nEpoch 18/20\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7724 - f1: 0.6689 - loss: 0.4873 - val_accuracy: 0.5968 - val_f1: 0.6600 - val_loss: 0.7911\nEpoch 19/20\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7674 - f1: 0.6626 - loss: 0.4898 - val_accuracy: 0.7295 - val_f1: 0.7197 - val_loss: 0.5967\nEpoch 20/20\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7720 - f1: 0.6843 - loss: 0.4891 - val_accuracy: 0.5240 - val_f1: 0.6314 - val_loss: 1.0413\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7d4e34fa1330>"},"metadata":{}}]},{"cell_type":"markdown","source":"Run all possible encodings with the get_model():","metadata":{}},{"cell_type":"code","source":"histories = {}\nfor mode in (\"int\", \"tf_idf\"):\n    for clean in (\"standard\", \"custom\"):\n        for ngrams in [(1,), (1,2)]:\n            name = \"__\".join([mode, clean, str(ngrams)])\n            print()\n            print(\"------------- Starting \" + name + \" ---------------------\")\n            X_train, y_train, X_val, y_val, X_test = preprocess_df(train_df, val_df, df_test=test_df,\n                                                                   mode=mode, clean=clean, ngrams=ngrams)\n            model = get_model(MAX_TOKENS, mode=mode)\n            #model.summary()\n            histories[name] = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=42, batch_size=64,\n                 callbacks=[keras.callbacks.ModelCheckpoint(name+\".keras\", save_best_only=True)])\n            \n            # Save predictions with best model:\n            model = keras.models.load_model(name+\".keras\")\n            predict_and_save(model, X_test, \"predictions__\" + name)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:31:29.822797Z","iopub.execute_input":"2024-08-13T12:31:29.824048Z","iopub.status.idle":"2024-08-13T12:32:59.405457Z","shell.execute_reply.started":"2024-08-13T12:31:29.824008Z","shell.execute_reply":"2024-08-13T12:32:59.403797Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"\n------------- Starting int__standard__(1,) ---------------------\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.5592 - f1: 0.3962 - loss: 0.6896 - val_accuracy: 0.5739 - val_f1: 0.0000e+00 - val_loss: 0.6816\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n\n------------- Starting int__standard__(1, 2) ---------------------\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.5670 - f1: 0.0000e+00 - loss: 0.6881 - val_accuracy: 0.5739 - val_f1: 0.0000e+00 - val_loss: 0.6804\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n\n------------- Starting int__custom__(1,) ---------------------\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.5679 - f1: 0.0000e+00 - loss: 0.6869 - val_accuracy: 0.5739 - val_f1: 0.0000e+00 - val_loss: 0.6841\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n\n------------- Starting int__custom__(1, 2) ---------------------\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.5724 - f1: 0.0000e+00 - loss: 0.6891 - val_accuracy: 0.5739 - val_f1: 0.0000e+00 - val_loss: 0.6846\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n\n------------- Starting tf_idf__standard__(1,) ---------------------\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5804 - f1: 0.0851 - loss: 0.6571 - val_accuracy: 0.6960 - val_f1: 0.4559 - val_loss: 0.5388\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\n------------- Starting tf_idf__standard__(1, 2) ---------------------\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.6207 - f1: 0.4444 - loss: 0.7063 - val_accuracy: 0.7682 - val_f1: 0.6459 - val_loss: 0.4914\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\n------------- Starting tf_idf__custom__(1,) ---------------------\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5780 - f1: 0.3442 - loss: 0.6624 - val_accuracy: 0.6737 - val_f1: 0.3872 - val_loss: 0.5558\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n\n------------- Starting tf_idf__custom__(1, 2) ---------------------\n\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6054 - f1: 0.2998 - loss: 0.6628 - val_accuracy: 0.7354 - val_f1: 0.5596 - val_loss: 0.5169\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"best_model_name = \"\"\nglobal_best_val_score = 0\n\nfor name, history in histories.items():\n    print(name)\n    mode, cleaning, ngrams = name.split(\"__\")\n    print(f\"Validation-f1-scores for {mode}-encoding, {cleaning}-cleaning, {ngrams}-ngrams:\")\n    vals_f1 = tf.stack(history.history[\"val_f1\"])[:,0].numpy() # Don't know why it is saved so intricated\n    print(vals_f1)\n    max_val = np.max(vals_f1)\n    if max_val > global_best_val_score:\n        global_best_val_score = max_val\n        best_model_name = name\n    print(\"The maximal f1-score: \", np.max(vals_f1))\n    print()\nprint(\"The best model in total is \", best_model_name)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:32:59.618575Z","iopub.execute_input":"2024-08-13T12:32:59.619085Z","iopub.status.idle":"2024-08-13T12:32:59.638921Z","shell.execute_reply.started":"2024-08-13T12:32:59.619042Z","shell.execute_reply":"2024-08-13T12:32:59.637613Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"int__standard__(1,)\nValidation-f1-scores for int-encoding, standard-cleaning, (1,)-ngrams:\n[0.]\nThe maximal f1-score:  0.0\n\nint__standard__(1, 2)\nValidation-f1-scores for int-encoding, standard-cleaning, (1, 2)-ngrams:\n[0.]\nThe maximal f1-score:  0.0\n\nint__custom__(1,)\nValidation-f1-scores for int-encoding, custom-cleaning, (1,)-ngrams:\n[0.]\nThe maximal f1-score:  0.0\n\nint__custom__(1, 2)\nValidation-f1-scores for int-encoding, custom-cleaning, (1, 2)-ngrams:\n[0.]\nThe maximal f1-score:  0.0\n\ntf_idf__standard__(1,)\nValidation-f1-scores for tf_idf-encoding, standard-cleaning, (1,)-ngrams:\n[0.45593417]\nThe maximal f1-score:  0.45593417\n\ntf_idf__standard__(1, 2)\nValidation-f1-scores for tf_idf-encoding, standard-cleaning, (1, 2)-ngrams:\n[0.64593774]\nThe maximal f1-score:  0.64593774\n\ntf_idf__custom__(1,)\nValidation-f1-scores for tf_idf-encoding, custom-cleaning, (1,)-ngrams:\n[0.3871763]\nThe maximal f1-score:  0.3871763\n\ntf_idf__custom__(1, 2)\nValidation-f1-scores for tf_idf-encoding, custom-cleaning, (1, 2)-ngrams:\n[0.5595628]\nThe maximal f1-score:  0.5595628\n\nThe best model in total is  tf_idf__standard__(1, 2)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Submit","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"predictions__\" + best_model_name)\ndf.to_csv(\"submission.csv\", index=False)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:35:08.771451Z","iopub.execute_input":"2024-08-13T12:35:08.771861Z","iopub.status.idle":"2024-08-13T12:35:08.798150Z","shell.execute_reply.started":"2024-08-13T12:35:08.771833Z","shell.execute_reply":"2024-08-13T12:35:08.796542Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"   id  target\n0   0       1\n1   2       1\n2   3       0\n3   9       0\n4  11       1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Helper function for recunstructing Sentences","metadata":{}},{"cell_type":"code","source":"def reconstruct_sentence(int_vec, vectorizer):\n    voc = np.array(vectorizer.get_vocabulary())\n    #print(int_vecs)\n    #print(type(int_vecs))\n    sentence = (\" \".join(voc[int_vec]))\n    return sentence\n#for i in range(...):\n#    print(reconstruct_sentence(...), \"  \", y_train[i])","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:35:23.152689Z","iopub.execute_input":"2024-08-13T12:35:23.153125Z","iopub.status.idle":"2024-08-13T12:35:23.159247Z","shell.execute_reply.started":"2024-08-13T12:35:23.153091Z","shell.execute_reply":"2024-08-13T12:35:23.157996Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"It seems, that our own original submission where heavily overfitted.\n\nUsing the ModelCheckpoint-Callback helps a lot! (What else do we have the validation data for?!)\n\nAlso, Justino's training starts to improve MUCH earlier, probably because of the ordering in the data, leading to some kind of \"Curriculum learning\". \n(See the code line with\ndf_equalized = pd.concat([class_0_under, class_1]))","metadata":{}}]}