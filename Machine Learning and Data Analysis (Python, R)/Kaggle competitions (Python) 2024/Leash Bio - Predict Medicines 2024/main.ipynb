{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":67356,"databundleVersionId":8006601,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction","metadata":{}},{"cell_type":"markdown","source":"**Using the \"Hash-Trick\": Calculate Fingerprints for Molecules and build a neural network for prediction of the binding affinity.**\n\nLink to the competition and data: https://www.kaggle.com/competitions/leash-BELKA/overview\n\n**Finding a good topology for the net still poses a big challenge.**\n\n**Since the dataset is too big, we need to provide a data stream.\nI thought it might be better to prepare TFRecord-Files to speed up the data supply.\nBut the data is even too much for my 20GB disc memory on Kaggle. So using a datastream from the provided files may be the only option to train on all of the data.**\n\n**One can try to use TPUs instead of CPU/GPU. But one may end up waiting in line. (The TPU-code may be commented.)**\n\n**IDEAS TO IMPROVE:**\n- Search for pretrained nets online. (There are many publications on this subject.)\n- Use Graph Neural Networks\n- Use other finger prints (e.g. 3D-finger prints)\n- Build three independent models: one for each target protein. (May be much easier to learn separate interaction patterns.) (---> Already done! See \"split_model()\".)","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nimport numpy as np\n\n# rdkit helps generating characteristics of molecules:\n!pip install rdkit\nimport rdkit\nimport rdkit.Chem as Chem\nfrom rdkit.Chem import AllChem\n\ngpu_name = tf.test.gpu_device_name()\nif \"GPU\" not in gpu_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(gpu_name))","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:23:33.153838Z","iopub.execute_input":"2024-06-14T05:23:33.154286Z","iopub.status.idle":"2024-06-14T05:23:48.237756Z","shell.execute_reply.started":"2024-06-14T05:23:33.154254Z","shell.execute_reply":"2024-06-14T05:23:48.235942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Switches:","metadata":{}},{"cell_type":"code","source":"# Original data gets preprocessed and saved into TFRecord files.\n# Otherwise use only csv\nuse_tfRecords = True\n\n# If submitting this file, different parameters will be used\nsubmit = False","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:23:48.240929Z","iopub.execute_input":"2024-06-14T05:23:48.241327Z","iopub.status.idle":"2024-06-14T05:23:48.247140Z","shell.execute_reply.started":"2024-06-14T05:23:48.241292Z","shell.execute_reply":"2024-06-14T05:23:48.245957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Select Hyperparameters","metadata":{}},{"cell_type":"code","source":"# Parameters for testing (small numbers):\n\nN_BITS_FINGERPRINT = 5 # for ECFP-Fingerprint\nN_RADIUS = 8 # for ECFP-Fingerprint\nBATCH_SIZE = 3000\nN_TRAIN = 6400#-1 # set to -1 for \"all\"\nN_TEST = 1000#-1 # set to -1 for \"all\"\nN_EPOCHS = 20\nWITH_DROPOUT = False\nDROPOUT_RATE = 0.05\nRESAMPLE_INSTEAD_OF_REWEIGHT = False\n\n#Numbers of neurons per internal layer:\nN_Layer_1 = 512\nN_Layer_2 = 30\n#N_Layer_3 = 10\n#N_Layer_4 = 5\n\n# Parameters for submissions (larger numbers):\nif submit:\n    BATCH_SIZE = 1000*5 # should be big, if (not RESAMPLE_INSTEAD_OF_REWEIGHT) in order to have at least some positive samples in the batch?\n    N_BITS_FINGERPRINT = 2048 # for ECFP-Fingerprint\n    N_RADIUS = 8 # for ECFP-Fingerprint\n    N_TEST = -1 # -1 means \"all\"\n    N_TRAIN = 1500000\n    N_EPOCHS = 15\n    N_Layer_1 = 2048\n    N_Layer_2 = 5\n    WITH_DROPOUT = False\n    DROPOUT_RATE = 0.05 # only needed if WITH_DROPOUT\n    RESAMPLE_INSTEAD_OF_REWEIGHT = False # both are ways of treating the imbalanced data","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:23:48.248613Z","iopub.execute_input":"2024-06-14T05:23:48.248977Z","iopub.status.idle":"2024-06-14T05:23:48.261480Z","shell.execute_reply.started":"2024-06-14T05:23:48.248948Z","shell.execute_reply":"2024-06-14T05:23:48.260029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions for creating TFRecords File","metadata":{}},{"cell_type":"code","source":"train_path = '/kaggle/input/leash-BELKA/train.csv'\ntest_path = '/kaggle/input/leash-BELKA/test.csv'\npreproc_train_path = \"/kaggle/working/train_prepro.tfrecord\"\npreproc_test_path = \"/kaggle/working/test_prepro.tfrecord\"","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:23:48.264032Z","iopub.execute_input":"2024-06-14T05:23:48.264428Z","iopub.status.idle":"2024-06-14T05:23:48.273574Z","shell.execute_reply.started":"2024-06-14T05:23:48.264396Z","shell.execute_reply":"2024-06-14T05:23:48.272239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_ecfp(molecule, radius=N_RADIUS, bits=N_BITS_FINGERPRINT):\n    return list(AllChem.GetMorganFingerprintAsBitVect(molecule, radius, nBits=bits))","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:23:48.275064Z","iopub.execute_input":"2024-06-14T05:23:48.275504Z","iopub.status.idle":"2024-06-14T05:23:48.285489Z","shell.execute_reply.started":"2024-06-14T05:23:48.275472Z","shell.execute_reply":"2024-06-14T05:23:48.284182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_preproc_file(path_in, path_out, n_entries = -1):\n    ### Calculates the fingerprint of each molecule\n    ### and saves this into the TFRecords file.\n\n    # The following functions can be used to convert a value to a type compatible\n    # with tf.train.Example.\n    def _bytes_feature(value):\n      \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n      if isinstance(value, type(tf.constant(0))):\n        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n      return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n    def _float_feature(value):\n        \"\"\"Returns a float_list from a float / double.\"\"\"\n        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n    def _int64_feature(value):\n        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n    \n    def serialize_example(id_, hash_, protein_name, binds=None):\n        features = {\n            'id': _int64_feature(id_),\n            'hash': _bytes_feature(tf.io.serialize_tensor(tf.constant(hash_))),\n            'protein_name': _bytes_feature(str.encode(protein_name)),\n          }\n        if binds!=None:\n            features['binds'] = _int64_feature(binds)\n        example = tf.train.Example(features=tf.train.Features(feature=features))\n        return example.SerializeToString()\n\n    # The actual creation of the file:\n    try:\n        os.remove(path_out)\n    except FileNotFoundError:\n        pass\n    with open(path_in) as f_in, tf.io.TFRecordWriter(path_out) as writer:\n        first_line = f_in.readline() # skip headers\n        labeled = (len(first_line.split(\",\")))==7\n        for i, line in enumerate(f_in):\n            if i==n_entries:\n                break\n            if i % 10000 == 0:\n                print(i, \"examples processed\")\n            features = line.split(\",\")\n            molecule = Chem.MolFromSmiles(features[4])\n            ecfp = generate_ecfp(molecule)\n            if labeled:\n                example = serialize_example(int(features[0]),\n                                        ecfp,\n                                        features[5],\n                                        int(features[6]))\n                writer.write(example)\n            else:\n                example = serialize_example(int(features[0]),\n                                        ecfp,\n                                        features[5])\n                writer.write(example)\n        print('done processing examples')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:23:48.287086Z","iopub.execute_input":"2024-06-14T05:23:48.287528Z","iopub.status.idle":"2024-06-14T05:23:48.310563Z","shell.execute_reply.started":"2024-06-14T05:23:48.287484Z","shell.execute_reply":"2024-06-14T05:23:48.309140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The following functions can be used to convert a value to a type compatible\n# with tf.train.Example.\ndef _bytes_feature(value):\n  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n  if isinstance(value, type(tf.constant(0))):\n    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\ndef _float_feature(value):\n    \"\"\"Returns a float_list from a float / double.\"\"\"\n    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\ndef _int64_feature(value):\n    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\ndef serialize_example(id_, hash_, protein_name, binds=None):\n    features = {\n        'id': _int64_feature(id_),\n        'hash': _bytes_feature(tf.io.serialize_tensor(tf.constant(hash_))),\n        'protein_name': _bytes_feature(str.encode(protein_name)),\n      }\n    if binds!=None:\n        features['binds'] = _int64_feature(binds)\n    example = tf.train.Example(features=tf.train.Features(feature=features))\n    return example.SerializeToString()","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:23:48.312682Z","iopub.execute_input":"2024-06-14T05:23:48.313451Z","iopub.status.idle":"2024-06-14T05:23:48.326202Z","shell.execute_reply.started":"2024-06-14T05:23:48.313401Z","shell.execute_reply":"2024-06-14T05:23:48.324578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def try_deleting(file_paths):\n    for file in file_paths:\n        try:\n            os.remove(file)\n        except FileNotFoundError:\n            pass","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:23:48.327620Z","iopub.execute_input":"2024-06-14T05:23:48.328005Z","iopub.status.idle":"2024-06-14T05:23:48.338731Z","shell.execute_reply.started":"2024-06-14T05:23:48.327974Z","shell.execute_reply":"2024-06-14T05:23:48.336708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_preproc_files_separate(path_in, path_out, n_entries = -1):\n    ### Calculates the fingerprint of each molecule\n    ### and saves this into the TFRecords files: one for binds and one for binds not.\n\n    # paths_out = [path_out + a for a in (\"_binds\", \"_binds_not\")]\n    try_deleting([path_out + a for a in (\"_binds\", \"_binds_not\")])\n    with open(path_in) as f_in, tf.io.TFRecordWriter(path_out+\"_binds\") as writer_binds, tf.io.TFRecordWriter(path_out+\"_binds_not\") as writer_binds_not:\n        first_line = f_in.readline() # skip headers\n        labeled = (len(first_line.split(\",\")))==7 # the bind-column is the 6th column\n        for i, line in enumerate(f_in):\n            if i==n_entries:\n                break\n            if i % 10000 == 0:\n                print(i, \"examples processed ...\")\n            features = line.split(\",\")\n            molecule = Chem.MolFromSmiles(features[4])\n            ecfp = generate_ecfp(molecule)\n            protein_name = features[5]\n            binds = int(features[6])\n            if labeled:\n                example = serialize_example(int(features[0]),\n                                            ecfp,\n                                            protein_name,\n                                            binds)\n                if binds:\n                    writer_binds.write(example)\n                else:\n                    writer_binds_not.write(example)\n            else:\n                example = serialize_example(int(features[0]),\n                                        ecfp,\n                                        protein_name)\n                writer.write(example)\n        print('done processing', i, ' examples')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:23:48.341943Z","iopub.execute_input":"2024-06-14T05:23:48.343012Z","iopub.status.idle":"2024-06-14T05:23:48.364910Z","shell.execute_reply.started":"2024-06-14T05:23:48.342940Z","shell.execute_reply":"2024-06-14T05:23:48.363608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions for reading the TFRecords","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef _parse_function(example, label=True):\n    # Set label=False if processing test data.\n    # Parse the input `tf.train.Example` proto using the dictionary:\n    feature_description = {\n      'id':           tf.io.FixedLenFeature([], tf.int64),\n      'hash':         tf.io.FixedLenFeature([], tf.string),\n      'protein_name': tf.io.FixedLenFeature([], tf.string),\n    }\n    if label:\n        feature_description['binds'] = tf.io.FixedLenFeature([1], tf.int64)\n    \n    features = tf.io.parse_single_example(example, feature_description)\n    id_     = features['id']\n    hash_   = tf.io.parse_tensor(features['hash'], out_type=tf.int32)\n    hash_   = tf.reshape(hash_, shape=(N_BITS_FINGERPRINT,))  # needed to bring back the shape to former string-byte\n    protein_name = features['protein_name']\n    if label:\n        return id_, hash_, protein_name, features['binds']\n    else:\n        return id_, hash_, protein_name\n\ndef _parse_hash_oneHot_binds(example_proto):\n    ### Returns only the Fingerprint(hash), the one-Hot-encoding of the protein and the binds-value\n    id_, hash_, protein_name, binds = _parse_function(example_proto)\n    oneHot = tf.math.equal(protein_name, ['BRD4', 'sEH', 'HSA'])\n    oneHot   = tf.reshape(oneHot, (3,))\n    return {'hash': hash_, 'oneHot': oneHot}, binds\n\ndef _parse_hash_oneHot(example_proto):\n    ### Returns only the Fingerprint(hash) and the one-Hot-encoding of the protein\n    id_, hash_, protein_name, = _parse_function(example_proto, label=False)\n    oneHot = tf.math.equal(protein_name, ['BRD4', 'sEH', 'HSA'])\n    oneHot   = tf.reshape(oneHot, (3,))\n    return {'hash': hash_, 'oneHot': oneHot}\n\ndef _parse_hash_binds(example_proto):\n    ### Returns only the Fingerprint(hash) and the binds-value\n    id_, hash_, protein_name, binds = _parse_function(example_proto)\n    return hash_, binds","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:23:48.372576Z","iopub.execute_input":"2024-06-14T05:23:48.373562Z","iopub.status.idle":"2024-06-14T05:23:48.395130Z","shell.execute_reply.started":"2024-06-14T05:23:48.373486Z","shell.execute_reply":"2024-06-14T05:23:48.393490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions for reading csv","metadata":{}},{"cell_type":"code","source":"@tf.py_function(Tout=tf.int32)\ndef get_fp(smile):\n    mol = Chem.MolFromSmiles(smile.numpy())\n    fp = generate_ecfp(mol)\n    fp = tf.constant(fp, dtype=tf.int32, shape=(N_BITS_FINGERPRINT,))\n    return fp\n\ndef _parse_csv_hash_oneHot_binds(x,y):\n    oneHot = tf.math.equal(x['protein_name'], ['BRD4', 'sEH', 'HSA'])\n    oneHot   = tf.reshape(oneHot,(3,))\n    hash_ = get_fp(x['molecule_smiles'])\n    hash_   = tf.reshape(hash_, shape=(N_BITS_FINGERPRINT,))\n    y = tf.reshape(y, shape=(1,))\n    y = tf.cast(y, tf.int64)\n    return {'hash': hash_, 'oneHot': oneHot}, y\n\ndef _parse_csv_hash_oneHot(x):\n    oneHot = tf.math.equal(x['protein_name'], ['BRD4', 'sEH', 'HSA'])\n    oneHot   = tf.reshape(oneHot,(3,))\n    hash_ = get_fp(x['molecule_smiles'])\n    return {'hash': hash_, 'oneHot': oneHot}","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:23:48.397005Z","iopub.execute_input":"2024-06-14T05:23:48.397906Z","iopub.status.idle":"2024-06-14T05:23:48.414078Z","shell.execute_reply.started":"2024-06-14T05:23:48.397858Z","shell.execute_reply":"2024-06-14T05:23:48.412241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get Dataset from either csv or TFrecords:","metadata":{}},{"cell_type":"code","source":"def get_ds_tfRecord(csv_path, tfRecords_path, batch_size, n_entries=-1, labeled=True, separate=False):\n    # creates the tfRecords-File from the csv-File\n    # n_entries=-1 means that all samples are to be taken from the csv\n    #\n    # separate=True means: first, two datasets for \"binds\" and \"binds\" not are created\n    # in order to merge them afterwards --> will reduce the data bias\n    \n    # determines automatically if labeled==True: (could be changed!)\n    if separate:\n        create_preproc_files_separate(csv_path,\n                        tfRecords_path,\n                        n_entries = n_entries)\n        ds_binds = tf.data.TFRecordDataset(tfRecords_path+\"_binds\")\n        ds_binds_not = tf.data.TFRecordDataset(tfRecords_path+\"_binds_not\")\n        if labeled:\n            ds_binds = ds_binds.map(_parse_hash_oneHot_binds)\n            ds_binds_not = ds_binds_not.map(_parse_hash_oneHot_binds)\n        else:\n            ds_binds = ds_binds.map(_parse_hash_oneHot)\n            ds_binds_not = ds_binds_not.map(_parse_hash_oneHot)\n        ds_binds = ds_binds.repeat()\n        ds_binds_not = ds_binds_not.repeat()\n        ds = tf.data.Dataset.zip((ds_binds, ds_binds_not)).flat_map(\n            lambda x,y : tf.data.Dataset.from_tensors(x).concatenate(tf.data.Dataset.from_tensors(y)))\n        ds = ds.shuffle(batch_size)\n        ds = ds.batch(batch_size)\n    else:\n        create_preproc_file(csv_path,\n                        tfRecords_path,\n                        n_entries = n_entries)\n        ds = tf.data.TFRecordDataset(tfRecords_path)\n        if labeled:\n            ds = ds.map(_parse_hash_oneHot_binds)\n        else:\n            ds = ds.map(_parse_hash_oneHot)\n        ds = ds.batch(batch_size)\n        ds = ds.repeat()\n        ds = ds.shuffle(batch_size)\n    return ds\n\ndef get_ds_csv(csv_path, batch_size, n_samples, labeled=True):\n    ### n_samples=-1 means \"take all\"\n    if labeled:\n        ds = tf.data.experimental.make_csv_dataset(\n            csv_path,\n            batch_size=4, # arbitrary\n            shuffle=False,\n            num_epochs=1, # to prevent repeat()\n            label_name='binds')\n    else:\n        ds = tf.data.experimental.make_csv_dataset(\n            csv_path,\n            batch_size=4,   # arbitrary\n            num_epochs=1, # to prevent repeat()\n            shuffle=False)\n    ds = ds.unbatch()\n    if n_samples!=-1:\n        ds = ds.take(n_samples)\n    if labeled:\n        ds = ds.map(_parse_csv_hash_oneHot_binds)\n    else:\n        ds = ds.map(_parse_csv_hash_oneHot)\n    ds = ds.batch(batch_size)\n    return ds\n\nif use_tfRecords:\n    print('Creating TFRecords file...')\n    ds = get_ds_tfRecord(train_path,\n                         preproc_train_path,\n                         batch_size=BATCH_SIZE,\n                         n_entries=N_TRAIN,\n                         separate=RESAMPLE_INSTEAD_OF_REWEIGHT)\nelse:\n    print('Use CSV')\n    with tf.device('/device:GPU:0'):\n        ds = get_ds_csv(train_path, batch_size=BATCH_SIZE, n_samples=N_TRAIN)\n\n#for elem in ds.take(1):\n#    print(elem)\n#    print(\" \")","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:23:48.416687Z","iopub.execute_input":"2024-06-14T05:23:48.417651Z","iopub.status.idle":"2024-06-14T05:24:00.575655Z","shell.execute_reply.started":"2024-06-14T05:23:48.417605Z","shell.execute_reply":"2024-06-14T05:24:00.574346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## First naive model","metadata":{}},{"cell_type":"code","source":"# Calculate proportion of positive binds\n# (Important for training weights)\nif submit:\n    n_consider = N_TRAIN\nelse:\n    n_consider = min(N_TRAIN, 20000)\nn_samples = ds.unbatch().take(n_consider).reduce(0, lambda i,_: i+1).numpy()\nn_binds = ds.unbatch().take(n_consider).reduce(0, lambda i, data: i+int(data[1])).numpy()[0]\nprint(f' {n_binds} of {n_samples} are positive')\nbinds_rate = n_binds / n_samples","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:24:00.577187Z","iopub.execute_input":"2024-06-14T05:24:00.577630Z","iopub.status.idle":"2024-06-14T05:24:01.842066Z","shell.execute_reply.started":"2024-06-14T05:24:00.577589Z","shell.execute_reply":"2024-06-14T05:24:01.840832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def straight_model():\n    hash_inputs = tf.keras.Input((N_BITS_FINGERPRINT,), name='hash')\n    protein_inputs = tf.keras.Input((3,), name='oneHot')\n    inputs = tf.keras.layers.concatenate([hash_inputs, protein_inputs])\n    x = inputs\n    x = tf.keras.layers.Dense(N_Layer_1, activation=\"relu\")(x)\n    if WITH_DROPOUT:\n            x = tf.keras.layers.Dropout(DROPOUT_RATE)(x)\n    x = tf.keras.layers.Dense(N_Layer_2, activation=\"relu\")(x)\n    #x = tf.keras.layers.Dropout(0.3)(x)\n    #x = tf.keras.layers.Dense(N_Layer_3, activation=\"relu\")(x)\n    #x = tf.keras.layers.Dropout(0.3)(x)\n    #x = tf.keras.layers.Dense(N_Layer_4, activation=\"relu\")(x)\n    \n    outputs = tf.keras.layers.Dense(1)(x)\n    model = tf.keras.Model(inputs=[hash_inputs, protein_inputs], outputs=outputs)\n    #print(model.summary())\n    #loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n    loss_fn = tf.keras.losses.BinaryFocalCrossentropy(from_logits=True)\n    model.compile(optimizer='adam',\n                  loss=loss_fn,\n                  metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:24:01.843500Z","iopub.execute_input":"2024-06-14T05:24:01.843837Z","iopub.status.idle":"2024-06-14T05:24:01.854559Z","shell.execute_reply.started":"2024-06-14T05:24:01.843809Z","shell.execute_reply":"2024-06-14T05:24:01.853176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def splitted_model():\n    ### creates single models for each target protein and concatenates them together\n    \n    def model_for_one_protein():\n        hash_input = tf.keras.Input((N_BITS_FINGERPRINT,), name='hash')\n        x = tf.keras.layers.Dense(N_Layer_1, activation=\"sigmoid\")(hash_input)\n        #x = tf.keras.layers.Dropout(0.1)(x)\n        x = tf.keras.layers.Dense(N_Layer_2, activation=\"sigmoid\")(x)\n        #x = tf.keras.layers.Dropout(0.3)(x)\n        #x = tf.keras.layers.Dense(N_Layer_3, activation=\"relu\")(x)\n        #x = tf.keras.layers.Dropout(0.3)(x)\n        #x = tf.keras.layers.Dense(N_Layer_4, activation=\"relu\")(x)\n        output = tf.keras.layers.Dense(1)(x)\n        return tf.keras.Model(hash_input, output)\n    \n       \n    def route_single(inputs):\n        hash_input, protein_input = inputs\n        hash_input = tf.expand_dims(hash_input, 0)\n        #tf.print(protein_input)\n        #tf.print(hash_input)\n        #output = tf.case([\n        #    (tf.equal(protein_input[0], True), lambda: model_BRD4(hash_input)[0]),\n        #    (tf.equal(protein_input[1], True), lambda: model_HSA(hash_input)[0]),\n        #    (tf.equal(protein_input[2], True), lambda: model_sEH(hash_input)[0])\n        #], exclusive=True)\n        \n        output = tf.reduce_sum(\n            tf.keras.layers.Concatenate()([\n                model_BRD4(hash_input)[0],\n                model_HSA(hash_input)[0],\n                model_sEH(hash_input)[0]\n            ])*\n            protein_input\n            )\n        \n        #index = tf.reduce_sum(tf.cast(protein_input, tf.int32) * tf.constant([1, 2, 4]))\n        #tf.print(\"index:\", index)\n\n        #output = tf.case([\n        #    (tf.equal(index, 1), lambda: model_BRD4(hash_input)[0]),\n        #    (tf.equal(index, 2), lambda: model_HSA(hash_input)[0]),\n        #    (tf.equal(index, 4), lambda: model_sEH(hash_input)[0])\n        #], exclusive=True)\n        return output\n    def route_batch(inputs):\n        hash_input, protein_input = inputs\n        #tf.print(hash_input)\n        output = tf.map_fn(route_single, inputs, fn_output_signature=tf.float32)\n        return output\n    \n    hash_input = tf.keras.Input((N_BITS_FINGERPRINT,), name='hash')\n    protein_input = tf.keras.Input((3,), name='oneHot')\n    model_BRD4 = model_for_one_protein()(hash_input)\n    model_HSA  = model_for_one_protein()(hash_input)\n    model_sEH  = model_for_one_protein()(hash_input)\n    single_models = tf.keras.layers.Concatenate(axis=1)([model_BRD4, model_HSA, model_sEH])\n\n    output = tf.keras.layers.Dot(axes=[1,1])([protein_input, single_models])\n    #output = tf.keras.layers.Lambda(route_batch, output_shape=(None,1))([hash_input, protein_input])\n    model = tf.keras.Model(inputs=[hash_input, protein_input], outputs=output)\n    print(model.summary())\n    \n    #loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n    loss_fn = tf.keras.losses.BinaryFocalCrossentropy(from_logits=True)\n    model.compile(optimizer='adam',\n                  loss=loss_fn,\n                  metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:24:01.855989Z","iopub.execute_input":"2024-06-14T05:24:01.856416Z","iopub.status.idle":"2024-06-14T05:24:01.873394Z","shell.execute_reply.started":"2024-06-14T05:24:01.856383Z","shell.execute_reply":"2024-06-14T05:24:01.872115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if TPU is available:\n#tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#tf.tpu.experimental.initialize_tpu_system(tpu)\n#tpu_strategy = tf.distribute.TPUStrategy(tpu)\n#with tpu_strategy.scope():\n\nmirrored_strategy = tf.distribute.MirroredStrategy()\n\nif RESAMPLE_INSTEAD_OF_REWEIGHT:\n    class_weights = {0: 1, 1: 1/binds_rate}\nelse:\n    class_weights = {0: 0.5, 1: 0.5}\n\nwith mirrored_strategy.scope():\n    #model = straight_model()\n    model = splitted_model()\n    #print(model.summary())\n    model.fit(ds,\n              epochs=N_EPOCHS,\n              steps_per_epoch=N_TRAIN//BATCH_SIZE,\n              class_weight=class_weights\n             )\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:26:06.709686Z","iopub.execute_input":"2024-06-14T05:26:06.710192Z","iopub.status.idle":"2024-06-14T05:28:06.755791Z","shell.execute_reply.started":"2024-06-14T05:26:06.710139Z","shell.execute_reply":"2024-06-14T05:28:06.753621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you want to continue/retake training:","metadata":{}},{"cell_type":"code","source":"#model2 = tf.keras.models.clone_model(model)\n#model2.fit(hash_ds, epochs=10, class_weight={0: 1, 1: 1/binds_rate})","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:24:02.104532Z","iopub.status.idle":"2024-06-14T05:24:02.105016Z","shell.execute_reply.started":"2024-06-14T05:24:02.104802Z","shell.execute_reply":"2024-06-14T05:24:02.104822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict on Test Set","metadata":{}},{"cell_type":"code","source":"test_ds = get_ds_csv(test_path, batch_size=512, n_samples=N_TEST, labeled=False)\ny = model.predict(test_ds)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:24:02.107367Z","iopub.status.idle":"2024-06-14T05:24:02.107805Z","shell.execute_reply.started":"2024-06-14T05:24:02.107596Z","shell.execute_reply":"2024-06-14T05:24:02.107614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Write Submission File","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nyy = y[:,0]\nd = pd.DataFrame({'id': range(len(yy)), 'binds': yy})\nd['id'] = d['id'] + 295246830\nfrom scipy.special import expit, logit\nd['binds'] = expit(d['binds'])\nd.to_csv('submission.csv', index=False, header=True)\nprint(d['binds'].describe())","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:24:02.108890Z","iopub.status.idle":"2024-06-14T05:24:02.109484Z","shell.execute_reply.started":"2024-06-14T05:24:02.109218Z","shell.execute_reply":"2024-06-14T05:24:02.109247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.weights","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:24:02.110748Z","iopub.status.idle":"2024-06-14T05:24:02.111142Z","shell.execute_reply.started":"2024-06-14T05:24:02.110953Z","shell.execute_reply":"2024-06-14T05:24:02.110969Z"},"trusted":true},"execution_count":null,"outputs":[]}]}